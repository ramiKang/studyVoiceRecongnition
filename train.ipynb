{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883a68c-fb5a-4885-9792-2da908312141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==1.18.3 in /home/haram/.local/lib/python3.10/site-packages (1.18.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (2024.2.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (15.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (4.66.2)\n",
      "Requirement already satisfied: multiprocess in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (3.9.5)\n",
      "Requirement already satisfied: pandas in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (2.2.2)\n",
      "Requirement already satisfied: dill in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (0.22.2)\n",
      "Requirement already satisfied: xxhash in /home/haram/.local/lib/python3.10/site-packages (from datasets==1.18.3) (3.4.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/haram/.local/lib/python3.10/site-packages (from aiohttp->datasets==1.18.3) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/haram/.local/lib/python3.10/site-packages (from aiohttp->datasets==1.18.3) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/haram/.local/lib/python3.10/site-packages (from aiohttp->datasets==1.18.3) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/haram/.local/lib/python3.10/site-packages (from aiohttp->datasets==1.18.3) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/haram/.local/lib/python3.10/site-packages (from aiohttp->datasets==1.18.3) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/haram/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (3.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==1.18.3) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==1.18.3) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets==1.18.3) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/haram/.local/lib/python3.10/site-packages (from pandas->datasets==1.18.3) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets==1.18.3) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==1.18.3) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.17.0 in /home/haram/.local/lib/python3.10/site-packages (4.17.0)\n",
      "Requirement already satisfied: sacremoses in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (0.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (1.23.4)\n",
      "Requirement already satisfied: filelock in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (3.13.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.17.0) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.17.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.17.0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (4.66.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/haram/.local/lib/python3.10/site-packages (from transformers==4.17.0) (2023.12.25)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/haram/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.17.0) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.17.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.17.0) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.17.0) (3.3.2)\n",
      "Requirement already satisfied: joblib in /home/haram/.local/lib/python3.10/site-packages (from sacremoses->transformers==4.17.0) (1.4.0)\n",
      "Requirement already satisfied: click in /home/haram/.local/lib/python3.10/site-packages (from sacremoses->transformers==4.17.0) (8.1.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jiwer in /home/haram/.local/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/haram/.local/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /home/haram/.local/lib/python3.10/site-packages (from jiwer) (3.8.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.23.4 in /home/haram/.local/lib/python3.10/site-packages (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "# pip\n",
    "# !apt install git-lfs\n",
    "\n",
    "!pip install datasets==1.18.3\n",
    "!pip install transformers==4.17.0\n",
    "!pip install jiwer\n",
    "!pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb604ab-928f-47e8-940a-23c92d10a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7774c3-2f6b-4d05-9ef4-fb034a50d217",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce28ac88-ac80-4563-8d46-ff86bdc83f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Enable CUDA error logging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLㅇE_DEVICES\"] = '0,1,2,3,4,5,6,7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823e270-080f-44d8-a9ba-221445ca8c42",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005baeee-6a43-4ea9-9cc6-9dd49e529033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haram/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# import local dataset\n",
    "train_dataset = load_from_disk(\"./data/datasets/train\")\n",
    "eval_dataset = load_from_disk(\"./data/datasets/eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a773f79a-077c-480a-988b-9beb1159a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 무작위로 샘플 보여주는 함수 정의\n",
    "def show_random_elements(dataset, num_examples=7):\n",
    "    assert num_examples <= len(dataset)\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5403f02b-1fce-441d-bc10-18c6525b472c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92664\n",
      "11350\n"
     ]
    }
   ],
   "source": [
    "# 한 번 확인하기\n",
    "# print(\"===========train=================\")\n",
    "print(len(train_dataset))\n",
    "# show_random_elements(train_dataset)\n",
    "# print(\"===========eval=================\")\n",
    "# show_random_elements(eval_dataset)\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeb9594-8bcc-4a04-adc0-31d206ac873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"transcription\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription\"]).lower() + \" \"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e351fc-a7de-418d-8719-8eb7b12a7033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function remove_special_characters at 0x7f173f806950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "92664ex [00:04, 18795.51ex/s]\n",
      "11350ex [00:00, 19862.36ex/s]\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 제거된 것 적용\n",
    "train_dataset = train_dataset.map(remove_special_characters)\n",
    "eval_dataset = eval_dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24fd6f-2ec4-4c60-952c-9a3ce6684b30",
   "metadata": {},
   "source": [
    "### Tokenizer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df89ede-4a41-489e-aca6-02cf9af292dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "    # 메모리 관련 로직 처리\n",
    "    memory = psutil.virtual_memory()\n",
    "    threshold = 80\n",
    "\n",
    "    if memory.percent >= threshold:\n",
    "        print(\"메모리 위험 :\",memory.percent)\n",
    "        return None\n",
    "        \n",
    "    all_text = \" \".join(batch[\"transcription\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa40c04-62ff-4e4b-ab76-935e8e2a05bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.53ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.98ba/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_train = train_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_dataset.column_names)\n",
    "vocab_eval = eval_dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96064477-87a9-4b2e-8ae5-011dfec6d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_eval[\"vocab\"][0]))\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b657d95c-b967-4a59-93d4-5eb924ca18e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 0,\n",
       " 'n': 1,\n",
       " 't': 2,\n",
       " 'x': 3,\n",
       " 'c': 4,\n",
       " 'j': 5,\n",
       " 'e': 6,\n",
       " 'h': 7,\n",
       " 'u': 8,\n",
       " 'z': 9,\n",
       " 'p': 10,\n",
       " 'k': 11,\n",
       " 'i': 12,\n",
       " 's': 13,\n",
       " \"'\": 14,\n",
       " 'd': 15,\n",
       " 'w': 16,\n",
       " 'g': 17,\n",
       " 'y': 18,\n",
       " 'm': 19,\n",
       " 'f': 20,\n",
       " 'o': 21,\n",
       " 'q': 22,\n",
       " 'l': 23,\n",
       " 'r': 24,\n",
       " 'a': 25,\n",
       " 'v': 26,\n",
       " ' ': 27}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09649fa4-61f5-4109-94fd-f2d2c68d7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d689a24-0591-48c4-baff-e1fc93f7fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8817fe2c-8305-4852-b096-a1fb8b137de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab json 저장\n",
    "import json\n",
    "\n",
    "model_path = \"./model/wav2vec_second\"\n",
    "vocab_json = model_path+\"/vocab.json\"\n",
    "\n",
    "\n",
    "with open(vocab_json, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a67f77e-9bdc-4186-ba50-86ec6a3a5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 생성\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(vocab_json, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82aa1eee-d3d1-4865-8d97-5cb6ffcf5c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/wav2vec_second/tokenizer_config.json',\n",
       " './model/wav2vec_second/special_tokens_map.json',\n",
       " './model/wav2vec_second/vocab.json',\n",
       " './model/wav2vec_second/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266616a0-1127-4d59-95df-f8cad6d0a517",
   "metadata": {},
   "source": [
    "### model에 대한 특성 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a105b5be-14d2-4fab-a755-1d58f6d30592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f75561f5-43b5-4fae-ae46-f844c2d8580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d84ac9c-1f64-41e7-8035-125575807e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전처리 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8f1f69-7e27-4add-9bda-8be6d19bb3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/x_train/7294/86026/7294-86026-0025.flac'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc766939-1d28-4208-a205-9d1be64cda9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/x_eval/7294/86026/7294-86026-0001.flac'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1b18fb5-f73d-4efe-ad53-864272fd14b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the platform has been endorsed and published by the society for the preservation of the wild fauna of the british empire london which is an endorsement of far reaching importance \n",
      "Target text: the platform has been endorsed and published by the society for the preservation of the wild fauna of the british empire london which is an endorsement of far reaching importance \n",
      "Input array shape: (181120,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rand_int = random.randint(0, len(train_dataset))\n",
    "\n",
    "print(train_dataset[rand_int][\"transcription\"])\n",
    "ipd.Audio(data=np.asarray(train_dataset[rand_int][\"audio\"][\"array\"]), autoplay=False, rate=16000)\n",
    "\n",
    "print(\"Target text:\", train_dataset[rand_int][\"transcription\"])\n",
    "print(\"Input array shape:\", np.asarray(train_dataset[rand_int][\"audio\"][\"array\"]).shape)\n",
    "print(\"Sampling rate:\", train_dataset[rand_int][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c514d74e-fb42-4f66-a486-1a08bd587f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no and yet you will let me perish with hunger ah that is a different thing well then wretches cried danglars i will defy your infamous calculations \n",
      "Target text: no and yet you will let me perish with hunger ah that is a different thing well then wretches cried danglars i will defy your infamous calculations \n",
      "Input array shape: (185200,)\n",
      "Sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(eval_dataset))\n",
    "\n",
    "print(eval_dataset[rand_int][\"transcription\"])\n",
    "ipd.Audio(data=np.asarray(eval_dataset[rand_int][\"audio\"][\"array\"]), autoplay=False, rate=16000)\n",
    "\n",
    "print(\"Target text:\", eval_dataset[rand_int][\"transcription\"])\n",
    "print(\"Input array shape:\", np.asarray(eval_dataset[rand_int][\"audio\"][\"array\"]).shape)\n",
    "print(\"Sampling rate:\", eval_dataset[rand_int][\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ad64749-6e58-465c-8e2d-53508aa2e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3983ae-ff30-4566-9556-22c5be2f2c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0ex [00:00, ?ex/s]2024-04-27 03:10:05.517339: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-27 03:10:05.565511: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-27 03:10:05.565555: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-27 03:10:05.565593: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-27 03:10:05.576086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "92664ex [15:33, 99.22ex/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_values', 'input_length', 'labels'],\n",
      "    num_rows: 92664\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dffe128f-4032-4728-a8d2-d85dc6b522d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11350ex [01:52, 101.28ex/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_values', 'input_length', 'labels'],\n",
      "    num_rows: 11350\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=eval_dataset.column_names)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "405b83f8-17cf-415c-b391-d08e9fef1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_length_in_sec =.0\n",
    "# train_dataset = train_dataset.filter(lambda x: x > max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780a682-dbc7-4fe6-ae64-50fd6c93a517",
   "metadata": {},
   "source": [
    "### trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa86c4d4-66ae-475c-843b-c0fc9f49b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abbc64b3-f0c3-4d35-84a4-c8e9e52b6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56925ddd-df41-4fc2-a997-998c563d97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10243577-f367-4402-b1b0-3931bc2405ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16779715-59ac-4702-adb9-2ed68042c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haram/.local/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"./model/wav2vec\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "190db972-67bc-4952-8476-0bc25e10a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a03a92bc-ff3e-420e-908a-30978b971e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # output_dir=repo_name,\n",
    "  output_dir=model_path,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  fp16=True,\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=300,\n",
    "  eval_steps=300,\n",
    "  logging_steps=300,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9033738f-b71a-4cd5-a71a-e50b394bb041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92664\n",
      "11350\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "374aace2-e230-404f-a316-195de75a3164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36254496-7dfb-4769-82cd-34d6478a056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda72e4-099f-4a5b-a620-7939b10210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HYU-GPU",
   "language": "python",
   "name": "hyu-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
